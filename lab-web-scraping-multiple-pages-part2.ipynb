{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb7ae7c",
   "metadata": {},
   "source": [
    "# Lab | Web Scraping Multiple Pages\n",
    "\n",
    "### Instructions Part 2\n",
    "#### Practice web scraping. This is not involved with the GNOD project of the week\n",
    "\n",
    "As you've seen, scraping the internet is a skill that can get you all sorts of information. Here are some little challenges that you can try to gain more experience in the field. Open a new Jupyter notebook and scrape at least 3 of these sites.\n",
    "\n",
    "- Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page: url ='https://en.wikipedia.org/wiki/Python'\n",
    "- Find the number of titles that have changed in the United States Code since its last release point: url = 'http://uscode.house.gov/download/download.shtml'\n",
    "- List all language names and number of related articles in the order they appear in wikipedia.org: url = 'https://www.wikipedia.org/'\n",
    "- A list with the different kind of datasets available in data.gov.uk: url = 'https://data.gov.uk/'\n",
    "- Display the top 10 languages by number of native speakers stored in a pandas dataframe: url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b1190",
   "metadata": {},
   "source": [
    "# Part 2: Practice web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313b964",
   "metadata": {},
   "source": [
    "## 1. Arbitrary Wikipedia page of \"Python\" and create a list of links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b277679d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a83ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0ddc1",
   "metadata": {},
   "source": [
    "### Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced0c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Wikipedia page for \"Python\"\n",
    "url = 'https://en.wikipedia.org/wiki/Python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4064d65",
   "metadata": {},
   "source": [
    "This code performs the following steps:\n",
    "1. It sends an HTTP request to the Wikipedia page for \"Python\".\n",
    "2. If the request is successful, it parses the HTML content of the page.\n",
    "3. Then, it finds all the `<a>` (anchor) tags with the `href` attribute (these are the links).\n",
    "4. It filters and formats these links to create a full URL and adds them to a list.\n",
    "5. Finally, it prints out the list of URLs.\n",
    "\n",
    "The code includes a condition to filter out administrative links (like links to Wikipedia's editing pages) by checking if the `href` attribute contains a colon (`:`). This is a simple way to get more relevant content links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafab1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending a request to the URL\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551cb209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "https://en.wikipedia.org/wiki/Python\n",
      "https://en.wikipedia.org/wiki/Python\n",
      "https://en.wikipedia.org/wiki/Python\n",
      "https://en.wikipedia.org/wiki/Pythonidae\n",
      "https://en.wikipedia.org/wiki/Python_(genus)\n",
      "https://en.wikipedia.org/wiki/Python_(mythology)\n",
      "https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "https://en.wikipedia.org/wiki/CMU_Common_Lisp\n",
      "https://en.wikipedia.org/wiki/PERQ#PERQ_3\n",
      "https://en.wikipedia.org/wiki/Python_of_Aenus\n",
      "https://en.wikipedia.org/wiki/Python_(painter)\n",
      "https://en.wikipedia.org/wiki/Python_of_Byzantium\n",
      "https://en.wikipedia.org/wiki/Python_of_Catana\n",
      "https://en.wikipedia.org/wiki/Python_Anghelo\n",
      "https://en.wikipedia.org/wiki/Python_(Efteling)\n",
      "https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "https://en.wikipedia.org/wiki/Python_(automobile_maker)\n",
      "https://en.wikipedia.org/wiki/Python_(Ford_prototype)\n",
      "https://en.wikipedia.org/wiki/Python_(missile)\n",
      "https://en.wikipedia.org/wiki/Python_(nuclear_primary)\n",
      "https://en.wikipedia.org/wiki/Colt_Python\n",
      "https://en.wikipedia.org/wiki/Python_(codename)\n",
      "https://en.wikipedia.org/wiki/Python_(film)\n",
      "https://en.wikipedia.org/wiki/Monty_Python\n",
      "https://en.wikipedia.org/wiki/Python_(Monty)_Pictures\n",
      "https://en.wikipedia.org/wiki/Timon_of_Phlius\n",
      "https://en.wikipedia.org/wiki/Pyton\n",
      "https://en.wikipedia.org/wiki/Pithon\n"
     ]
    }
   ],
   "source": [
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Finding all the 'a' tags with 'href' attribute\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Creating a list to hold the URLs\n",
    "    urls = []\n",
    "\n",
    "    # Extracting URLs from the 'a' tags\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if href.startswith('/wiki/') and ':' not in href:  # Filtering out administrative links\n",
    "            full_url = f'https://en.wikipedia.org{href}'\n",
    "            urls.append(full_url)\n",
    "\n",
    "    # Printing the list of URLs\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "else:\n",
    "    print(f'Failed to retrieve the page. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868a2be",
   "metadata": {},
   "source": [
    "## 2.  Number of Titles Changed in the United States Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50616604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the webpage content\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d40272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Titles Changed: 1\n"
     ]
    }
   ],
   "source": [
    "# Find elements indicating changed titles (titles in bold)\n",
    "# Find class name\n",
    "changed_titles = soup.find_all('div', class_='usctitlechanged')  \n",
    "\n",
    "# Print the number of changed titles\n",
    "print(f'Number of Titles Changed: {len(changed_titles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8707bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i in soup.select('.usctitlechanged'):\n",
    "    titles.append(i.text.split('.')[0].strip().replace('\\n','').replace(' ٭',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145f6c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Title 38 - Veterans' Benefits\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3edb6c",
   "metadata": {},
   "source": [
    "## 3. List of Languages and Number of Articles on Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3c7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the webpage content\n",
    "url = 'https://www.wikipedia.org/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a12505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 6 744 000+ articles\n",
      "Español: 1 906 000+ artículos\n",
      "Русский: 1 947 000+ статей\n",
      "日本語: 1 392 000+ 記事\n",
      "Deutsch: 2 852 000+ Artikel\n",
      "Français: 2 567 000+ articles\n",
      "Italiano: 1 835 000+ voci\n",
      "中文: 1 387 000+ 条目 / 條目\n",
      "العربية: 1 221 000+ مقالة\n",
      "Português: 1 113 000+ artigos\n"
     ]
    }
   ],
   "source": [
    "# Find language elements\n",
    "languages = soup.find_all('a', class_='link-box')  \n",
    "\n",
    "# Iterate over each language element and extract information\n",
    "for language in languages:\n",
    "    lang_name = language.find('strong').text.strip()\n",
    "    num_articles = language.find('small').text.strip().replace('\\xa0', ' ')\n",
    "    print(f'{lang_name}: {num_articles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8f3a7",
   "metadata": {},
   "source": [
    "## 4. List of Datasets Available on data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d44499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the webpage content\n",
    "url = 'https://data.gov.uk/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31cf4b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business and economy', 'Crime and justice', 'Education', 'Health', 'Mapping', 'Towns and cities', 'Transport', 'Digital service performance', 'Government reference data']\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the dataset categories\n",
    "dataset_categories = []\n",
    "\n",
    "# Look for the HTML elements that contain the categories\n",
    "# They seem to be in <a> tags with a class that includes 'govuk-link'\n",
    "category_elements = soup.find_all(\"a\", class_='govuk-link')\n",
    "\n",
    "# Iterate through each <a> tag\n",
    "for element in category_elements:\n",
    "    # Check if the parent element has class 'govuk-grid-column-full'\n",
    "    if \"govuk-grid-column-full\" in element.find_parent(\"div\")[\"class\"]:\n",
    "        # Extract the text from the <a> tag\n",
    "        category_name = element.get_text().strip()\n",
    "        # Check if the text contains \"a\"\n",
    "        if category_name and \"a\" in category_name:\n",
    "            # Add the category name to our list\n",
    "            dataset_categories.append(category_name)\n",
    "\n",
    "# Print the list of dataset categories\n",
    "print(dataset_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fa8ed",
   "metadata": {},
   "source": [
    "## 5. Top 10 Languages by Number of Native Speakers in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c323e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the webpage content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f04c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers(millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>939</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>485</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>380</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>345</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>236</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>147</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>123</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yue Chinese</td>\n",
       "      <td>86.1</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Language Native speakers(millions) Language family        Branch\n",
       "0  Mandarin Chinese                       939    Sino-Tibetan       Sinitic\n",
       "1           Spanish                       485   Indo-European       Romance\n",
       "2           English                       380   Indo-European      Germanic\n",
       "3             Hindi                       345   Indo-European    Indo-Aryan\n",
       "4        Portuguese                       236   Indo-European       Romance\n",
       "5           Bengali                       234   Indo-European    Indo-Aryan\n",
       "6           Russian                       147   Indo-European  Balto-Slavic\n",
       "7          Japanese                       123         Japonic      Japanese\n",
       "8       Yue Chinese                      86.1    Sino-Tibetan       Sinitic\n",
       "9        Vietnamese                      85.0   Austroasiatic        Vietic"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on the table containing the data, it has the class 'wikitable sortable static-row-numbers'\n",
    "languages_table = soup.find('table', {'class': 'wikitable sortable static-row-numbers'})\n",
    "\n",
    "# Extract the rows from the table\n",
    "rows = languages_table.find_all('tr')\n",
    "\n",
    "# The first row is the header, the rest are the data rows\n",
    "header = [th.text.strip() for th in rows[0].find_all('th')]\n",
    "data_rows = rows[1:11]  # We want the top 10 entries\n",
    "\n",
    "# Extract data from each row\n",
    "data = []\n",
    "for row in data_rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append(cols)  # Add the cleaned column data to the data list\n",
    "\n",
    "# Create a DataFrame with the extracted data\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
